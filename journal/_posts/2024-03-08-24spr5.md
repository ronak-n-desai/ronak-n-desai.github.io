---
layout: post
title: Fuchs v2 Results, 03/11 Data
use_math: true
category: journal
---

# New Synthetic Data results
I decided to redo a lot of the synthetic data results. Here are some of the main changes I implemented
- Performing a Grid Search of Hyperparameters to have a better justification for model architecture using 5 fold CV.
- Implementing a correction factor to account for the log-scaling bias
- Using the log-gaussian noise instead of gaussian noise
- Changing the neural network to the skorch architecture with early stopping and validation set
- Changing GPR and SVR to a chained output regressor so that we don't have 3 completely independent models.
- Doing 40 data fraction splits instead of 20 to look at multiples of 500 training points (instead of 1000)
- Adding optimization plots that compare effectiveness of each model to predict a certain desired condition

## Exploratory Data Analysis

I made a scatter plot of all 25000 points in the noiseless fuchs dataset to understand the dependence on maximum energy on the three inputs (the total and average energy behave similarly):

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/bd358e15-c008-44a2-ab6b-b5647a57f525)

Here, we can see a roughly linear dependence with intensity, a negative quadratic dependence with focal distance, and inverse relationship with target thickness. Here, we can see that there are far more points with lower energy than there are with higher energy. Due to this, we suspect that the distribution of energies in the dataset is skewed, so I took the log of the output energies and plotted their distributions as a histogram:

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/c506c8aa-f000-4fef-b8f9-f12bd7ac7169)

Here, it is clear that the log of the energies are approximately normally distributed. This is the motivation behind the logarithmic scaling and in our tests it turned out to help the model to train much better.

## Grid Searches
I used GridSearchCV which scans over all combinations of specified hyperparameters and trains with 5 fold Cross-validation and picks the set of hyper-parameters with best CV score (i.e. lowest mean squared error averaged between the 5 CV splits). I used a fuchs dataset with 10 percent added noise and 5000 training points. In this way, we are not picking a new set of hyperparameters for different noise levels and numbers training points when we get to the training step so that we can see how resilient these models are when adding/removing points or changing the noise.

The final Neural network architecure I settled on included the following parameters:

- `batch_size = 256`
- `gamma = 0.95` (learning rate decay)
- `n_hidden_layers = 3`
- `n_neurons = 64` (per hidden layer)
- `learning_rate = 1e-3` (initial learning rate)

and I used the `Adam` optimizer, `MSELoss` loss function, `LeakyReLU` activation function because these have been working well for us for a while now. I used the default `patience` of 5 for the early stopping function. 

Using all these settings, I also made a figure comparing the effect of different model architectures (varying the number of hidden layers and number of neurons per layer)

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/efcd30d3-21f3-49bb-ae9f-3f9a3798e19e)

Here, it shows that the (64x3) architecture not only has the best CV score, but also the smallest uncertainty in its score.

The final SVR architecture I settled on included the following parameters:
- `C = 2.5` (Regularization Parameter)
- `epsilon = 1e-2` (Specifies tube within which no penalty is associated in loss function)
- `tol = 1e-3` (Tolerance for stopping criterion)

and I used the defaults of `gamma = scale` and `kernel = rbf`. 

The final GPR architecture I settled on included the following parameters: 
- `num_epochs = 30`
- `lr = 2e-1`

The grid search initially yielded a learning rate of `5e-1` but I found that this learning rate was a bit too high in the training process for other noises/splits which caused some numerical instabilities.
So, I settled with the smaller learning rate which also has the added advantage of a lower mean uncertainty estimation (calculated point by point) from the GPR fit of around $8.4 \%$ (compared to $11-12 \%$ for the other parameters). 
Below is the CV score comparison of the three models where we see the regression models fare better than the neural network model

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/d46dcf8d-1982-466a-adb8-b68b6ace1cf2)

## Model Training

From a time perspective, the results did not differ much : The GPR still takes a very long time and the SVR trains extremely quickly

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/4aa329f8-7ebc-4dba-85de-013d45972540)

and the GPR still consumes a lot of GPU memory.

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/ae366aed-744a-43ee-8106-ba396989a906)

which can be mitigated by using approximate methods like the SVGP. As far as model performance goes, the grid searches were a good indicator because the GPR > SVR > NN for the MAPE on noiseless testing data:

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/eb9d1628-d915-4b7c-8b3f-59548d3bce8b)

The following plots have the dashed lines as the noiseless testing MAPE and the solid lines as the noisy testing MAPE. We can compare the accuracy with 500 training points and 20000 training points of the models

| ![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/f7b0c32d-2853-42a1-92f0-80b68d97bb2f) | ![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/e0209b49-c9b9-4f95-a491-65f905be94cb) |

Additionally, I made a plot showing three levels of noise: 0, 15, and 30

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/8a792a7f-6bbd-49a3-9056-9e75a81b9944)

where one useful feature about the NN seems to be that it gets better or stays the same with more noise. With little noise, it performs much worse than the other two.

## Optimization

A big motivation for why the synthetic data training is important is because it can illustrate how a learned model can even output the noise and make good predictions. I designed an optimization computational experiment to compare the effectiveness of the models. I fixed the intensity to be the maximum and examined which points in the target thickness - focal distance space would have maximum proton energy closest to a specified cutoff but also laser to proton conversion efficiency (equivalently total proton energy in our model) to be as high as possible (so long as the maximum proton energy isn't more than $15 \%$ away from the specified cutoff). The following plot illustrates the region in phase space where these conditions are satisfied

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/14da600f-114a-420a-a7fd-a4b78921ce01)

where the gray lines are the $15 \%$ cutoff, the red lines indicate the exact energy cutoff, and the green region only maximizes the total energy. The blue region weights both maximizing the total energy and also keeping the max energy near the cutoff and is primarily the region of interest. The green and blue regions are within 5 percent of the optimal value indicated with the green and blue stars.

If we did not train a ML model and just plotted 2,000 points worth of data (with 30 percent added noise) we would get a plot like this: 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/c3bc4145-17ce-4e10-9191-de4ef91ee70a)

where the individual blue and green points fall within 5 percent of the blue and green stars. This plot is not very helpful, the predictions are all over the place because the high noise level and limited amount of data.

Instead, we can fit a ML model to 2,000 points of data which will smooth out the noise. Then, I made 100,000 prediction points from these trained models and evaluated the desired objective functions on each to make the plots seen below

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/8e70f8b1-238c-49fd-9a26-13c429ec83c1)

Here, the GPR seems to perform the best and the NN worst as expected. Even so, all models still provide a much better estimate than the raw data.













# AFRL Data from 03/11/24

## Target Data 
Since last time, we've added a new entry: `PZT Position`, which is mainly what Kyle has been adjusting for the target. 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/f4adb746-02f2-44bb-872d-4a30569502b9)

We can see this reflected in the graph above. Additionally, we can see that the UNIX times being recorded (also a new entry) are in pacific time. It also seems that even this UNIX time is off by around 30 seconds or so because the file creation timestamp is around (3 hours and) 35 seconds ahead of the first target data being written to file.


## Main Pulse and Prepulse
One big issue with the main pulse and prepulse data is that the shot times seem to be off between the prepulse and main pulse

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/0940e869-ef76-4161-96b6-8fca07bcc7a5)

If the shot time is unreliable (seems like up to 30 seconds unreliable), then an accurate mapping between shot number and shot time can't be established. Even in the very beginning, the prepulse and main pulse are off by 2.2 seconds and this drifts to 32.2 seconds closer to the end of the DAQ run. Also, the Diode Timestamps are in Pacific Time (and possibly not synced to XL to GO because the file creation time stamp is after the first time of the first shot which doesn't make sense to me).

For this DAQ run, we ran the main pulse looping program which varies just the main pulse intensity (where I plotted a rolling average to smooth things out).

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/307de674-41c2-46a0-8e80-8249dd69119c)

I also plotted the rolling average of the prepulse which should be 0 because the prepulse was turned off

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/6984bc99-e7c3-43b7-b2d7-5472203294fc)

## Espec and Pspec Data
We set the exposure time to 1ms which was too low and as a result, we didn't get any electron signals. The proton signals were probably the result of scatter light and xrays, we didn't see any proton signals after pixel number 900.

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/1cee7a7d-b8f8-4915-9868-7e7ea04e79bc)

Above, is a plot showing three of the highest electron signals which were just these skinny lines for pixel number greater than 900. Also we can see their corresponding readings from the Pspec camera. Additionally, I wanted to see if the triggers were freezing and if they were accurate between the two cameras. I plotted the difference in their trigger count and we didn't see any evidence of freezing (one trigger count going up while the other staying constant would mean that we'd see big jumps).

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/7f17e935-1db0-40b2-b8d3-71e0095e51bc)

However, I also wanted to look at the time stamp data as well just to confirm if the timestamps were freezing as well. I merged both Pspec and Espec dfs on the common axis of trigger count and plotted the difference in their timestamps (modulo 65535)

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/75270841-f64d-4e62-b730-dc3a9718bf8a)

To me, this plot shows a constant drift in time between the clocks of the Pspec and Espec at a rate of approximately (assuming 20Hz data collection) of 30ms drift per 1 second of data collection, which is a pretty significant amount.



## Delay and Waveplate Data

Kyle added some new entries to the pre-pulse delay file so that each file now has entries (separated by tabs) looking like this:

`Time` `P-P Pos` `MainWP` `P-P WP`

where the time is `HH:MM:SS` (not a unix time), `P-P Pos` is the time delay between pre pulse and main pulse (in ps), `MainWP` and `P-P WP` are the angles of the waveplates that control the main and prepulse intensity.

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/38420450-bd33-4be2-a3b0-1990d9f125e5)

We can see the looping program changing the angle on the waveplate for the Main Pulse. The Pre-Pulse waveplate position is not relevant in this run because the shutter was closed (which wouldn't show up on this plot). 

 















