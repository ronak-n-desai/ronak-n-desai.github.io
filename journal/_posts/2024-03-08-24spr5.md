---
layout: post
title: Fuchs v2 Results, AFRL real-time ML
use_math: true
category: journal
---

# New Synthetic Data results
I decided to redo a lot of the synthetic data results. Here are some of the main changes I implemented
- Performing a Grid Search of Hyperparameters to have a better justification for model architecture using 5 fold CV.
- Implementing a correction factor to account for the log-scaling bias
- Using the log-gaussian noise instead of gaussian noise
- Changing the neural network to the skorch architecture with early stopping and validation set
- Changing GPR and SVR to a chained output regressor so that we don't have 3 completely independent models.
- Doing 40 data fraction splits instead of 20 to look at multiples of 500 training points (instead of 1000)
- Adding optimization plots that compare effectiveness of each model to predict a certain desired condition

## Grid Searches

The final Neural network architecure I settled on included the following parameters:

- `batch_size = 256`
- `gamma = 0.95` (learning rate decay)
- `n_hidden_layers = 3`
- `n_neurons = 64` (per hidden layer)
- `learning_rate = 1e-3` (initial learning rate)

and I used the `Adam` optimizer, `MSELoss` loss function, `LeakyReLU` activation function because these have been working well for us for a while now. I used the default `patience` of 5 for the early stopping function. 

Using all these settings, I also made a figure comparing the effect of different model architectures (varying the number of hidden layers and number of neurons per layer)

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/efcd30d3-21f3-49bb-ae9f-3f9a3798e19e)


The final SVR architecture I settled on included the following parameters:
- `C = 2.5` (Regularization Parameter)
- `epsilon = 1e-2` (Specifies tube within which no penalty is associated in loss function)
- `tol = 1e-3` (Tolerance for stopping criterion)

and I used the defaults of `gamma = scale` and `kernel = rbf`. 

The final GPR architecture I settled on included the following parameters: 
- `num_epochs = 30`
- `lr = 2e-1`

The grid search initially yielded a learning rate of `5e-1` but I found that this learning rate was a bit too high in the training process for other noises/splits which caused some numerical instabilities.
So, I settled with the smaller learning rate which also has the added advantage of a lower mean uncertainty estimation (calculated point by point) from the GPR fit of around $8 \%% (compared to $11-12 \%$ for the other parameters). 



