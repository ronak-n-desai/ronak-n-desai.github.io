---
layout: post
title: Fuchs v2 Results
use_math: true
category: journal
---

# New Synthetic Data results
I decided to redo a lot of the synthetic data results. Here are some of the main changes I implemented
- Performing a Grid Search of Hyperparameters to have a better justification for model architecture using 5 fold CV.
- Implementing a correction factor to account for the log-scaling bias
- Using the log-gaussian noise instead of gaussian noise
- Changing the neural network to the skorch architecture with early stopping and validation set
- Changing GPR and SVR to a chained output regressor so that we don't have 3 completely independent models.
- Doing 40 data fraction splits instead of 20 to look at multiples of 500 training points (instead of 1000)
- Adding optimization plots that compare effectiveness of each model to predict a certain desired condition

## Exploratory Data Analysis

I made a scatter plot of all 25000 points in the noiseless fuchs dataset to understand the dependence on maximum energy on the three inputs (the total and average energy behave similarly):

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/bd358e15-c008-44a2-ab6b-b5647a57f525)

Here, we can see a roughly linear dependence with intensity, a negative quadratic dependence with focal distance, and inverse relationship with target thickness. Here, we can see that there are far more points with lower energy than there are with higher energy. Due to this, we suspect that the distribution of energies in the dataset is skewed, so I took the log of the output energies and plotted their distributions as a histogram:

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/c506c8aa-f000-4fef-b8f9-f12bd7ac7169)

Here, it is clear that the log of the energies are approximately normally distributed. This is the motivation behind the logarithmic scaling and in our tests it turned out to help the model to train much better.

## Grid Searches

The final Neural network architecure I settled on included the following parameters:

- `batch_size = 256`
- `gamma = 0.95` (learning rate decay)
- `n_hidden_layers = 3`
- `n_neurons = 64` (per hidden layer)
- `learning_rate = 1e-3` (initial learning rate)

and I used the `Adam` optimizer, `MSELoss` loss function, `LeakyReLU` activation function because these have been working well for us for a while now. I used the default `patience` of 5 for the early stopping function. 

Using all these settings, I also made a figure comparing the effect of different model architectures (varying the number of hidden layers and number of neurons per layer)

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/efcd30d3-21f3-49bb-ae9f-3f9a3798e19e)

Here, it shows that the (64x3) architecture not only has the best CV score, but also the smallest uncertainty in its score.

The final SVR architecture I settled on included the following parameters:
- `C = 2.5` (Regularization Parameter)
- `epsilon = 1e-2` (Specifies tube within which no penalty is associated in loss function)
- `tol = 1e-3` (Tolerance for stopping criterion)

and I used the defaults of `gamma = scale` and `kernel = rbf`. 

The final GPR architecture I settled on included the following parameters: 
- `num_epochs = 30`
- `lr = 2e-1`

The grid search initially yielded a learning rate of `5e-1` but I found that this learning rate was a bit too high in the training process for other noises/splits which caused some numerical instabilities.
So, I settled with the smaller learning rate which also has the added advantage of a lower mean uncertainty estimation (calculated point by point) from the GPR fit of around $8.4 \%$ (compared to $11-12 \%$ for the other parameters). 
Below is the CV score comparison of the three models where we see the regression models fare better than the neural network model

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/d46dcf8d-1982-466a-adb8-b68b6ace1cf2)

## Model Training

From a time perspective, the results did not differ much : The GPR still takes a very long time and the SVR trains extremely quickly

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/4aa329f8-7ebc-4dba-85de-013d45972540)

and the GPR still consumes a lot of GPU memory.

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/ae366aed-744a-43ee-8106-ba396989a906)

which can be mitigated by using approximate methods like the SVGP. As far as model performance goes, the grid searches were a good indicator because the GPR > SVR > NN for the MAPE on noiseless testing data:

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/eb9d1628-d915-4b7c-8b3f-59548d3bce8b)

The following plots have the dashed lines as the noiseless testing MAPE and the solid lines as the noisy testing MAPE. We can compare the accuracy with 500 training points and 20000 training points of the models

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/f7b0c32d-2853-42a1-92f0-80b68d97bb2f) ![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/e0209b49-c9b9-4f95-a491-65f905be94cb)



 















