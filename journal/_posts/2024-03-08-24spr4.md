---
layout: post
title: Log-Scaling Correction, Grid Searches
use_math: true
category: journal
---

# Log-Scaling Correction

## Log-Scaling Correction Derivation
As I've mentioned [previously](https://ronak-n-desai.github.io/24spr3/), the log-scaling skews the predictions of a ML model. In fact, it always makes the ML model under-predict the true function. 
The log normal distribution (see [wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution) page) is what I am using to add noise to the output energies. The relevant formula is 

\begin{equation}
  \mu = \log\left(\frac{\mu_X^2}{\sqrt{\mu_X^2 + \sigma_X^2}}\right)
\end{equation}

Here, $\mu_X$ would be the value of the output energy. After log-scaling, the model will ideally learn the output energy to be $\mu$. When we exponentiate the model prediction, we will be left with $\exp(\mu) < \mu_X$. This is the origin of the under-prediction

The PDF of the log-normal distrubution is 

\begin{equation}
  f(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \exp\left(- \frac{(\log(x) - \mu)^2}{2 \sigma^2} \right)
\end{equation}

where $\mu = \log\left(\frac{\mu_X}{\sqrt{1 + \alpha^2}} \right)$ is the mean of the logged distribution, $\sigma^2 = \log(1 + \alpha^2)$ is the variance of the logged distribution, and $\alpha = \sigma_X / \mu_X$ is the noise level of the original (log-normal) distribution. Additionally, $\mu_X$ and $\sigma_X$ are the mean and variance of the log-normal distribution. 

The expected value $E[x] = \mu_X$ can be confirmed by taking $\int_0^\infty x f(x)$ (I just used Mathematica). To get the expected value of the log of this distribution, we can take $\int Log(x) f(x) = \log\left(\frac{\mu_X}{\sqrt{1 + \alpha^2} } \right) = \mu$ which makes sense due to our definition of $\mu$. We can calculate the underprediction bias as a percentage as

\begin{equation}
\text{bias} = \frac{\mu_X - \mu^*}{\mu_X} \times 100 \% = \frac{\sqrt{1 + \alpha^2} - 1}{\sqrt{1 + \alpha^2}} \times 100 \%
\end{equation}

where $\mu^* \equiv e^\mu$

This bias is something that is discussed online. A simple explanation can be given in this [Medium](https://roizner.medium.com/when-logarithmic-scale-in-prediction-models-causes-bias-d80d84e9e3d5) article. An academic article is given by [Miller 1984](https://www.jstor.org/stable/2683247?origin=crossref&seq=2)

## Techniques to Implement Prediction Correction
To fix this underprediction bias, we would want to multiply the predictions by a correction factor equal to $\mu_X / \mu^*$ = \sqrt{1 + \alpha^2}$. However, this comes with two assumptions. 

1. The noise in our experiment is distributed log-normally
2. We know the constant noise level $\alpha$ exactly.

This multiplicative factor of $\sqrt{1 + \alpha^2}$ should work for our synthetic data, but may be difficult to apply to real experimental data where the amount of noise and noise model may not be known exactly or hard to determine. However, the important thing is that in this process we are developing a correction factor that is equal to the ratio of two quantities: (1) the mean of the un-logged distribution, (2) the exponential of the mean of the logged distribution. We can determine (1) from the experimental dataset and (2) from the training predictions on the experimental dataset. I explored three ways to come up with an effective correction factor. I will refer to the predictions as $y_P$ and the data as $y_D$. 

1. C = mean($y_D$) / mean($y_P$)
2. C = mean($y_D / y_P$)
3. Perform a linear least-squares fit $y_D = C y_P$ (setting the y-intercept to 0) to determine $C$

All three of these methods performed very similarly:

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/ceeef52a-53ff-476e-9d04-205886559d11)

The black dashed line shows the log-scaling bias $\sqrt{1 + \alpha^2}$



