---
layout: post
title: Improving DAQ scripts, Fuchs Paper Updates
use_math: true
category: journal
---

This post covers work from 03/25 - 04/05. I focused on improving the DAQ process at AFRL and also on getting the synthetic data ready for publication.

# Building an Regression on electron data

Even though the data we collected on 03/18 was limited, I wanted to see what we could learn from fitting a polynomial to the data. The first thing I did was to take all the data and plot a heat map and scatter plot of the number of electrons with MP and PP intensity as inputs

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/9640fa02-9cd1-4b36-bdf9-04c48acfa01f)

Here, there's no clear correlation. I did the same thing, but with the average pixel instead 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/217eb21d-4807-44d0-8899-b1d7275e5236)

Here, it's more clear that a higher pre pulse is desirable. One issue is that since our data collection biased higher main pulse and lower prepulse (can be seen in the correlation plots from the previous post), we aren't able to explore the parameter space fully.

Next, I trained a polynomial regression on the data (80 $\%$ of 4649 points). To better understand the relationship between the variables, First, I varied the MP while keeping the PP constant at 3 different chosen values and outputted the model predictions overlayed with corresponding data from our dataset

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/fce3fbc6-34e4-4820-b679-7bf29996c446)

To me, it appears that the data is really noisy, so its really hard to see a correlation. I did a similar thing, but instead varied the PP while keeping the MP constant to get a similar plot

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/cb28ee67-e0cf-4eb1-8715-18b076b9c271)

Ultimately, we need to collect more data to get a better result, the MP and PP data is just oo noisy. The good news is that Nathaniel recently changed the diode timings on 03/25 to make the MP and PP data less noisy. This should help significantly. Additionally, collecting data at 100Hz would give us 5x more data points to work with.

# Revising the Waveplate looping program

In the last post, I showed that there was an issue with the waveplate looping. If I look at the correlation plots from last time, it looks like the main pulse is biased to be larger and the pre pulse is biased to be smaller. This can be seen by taking the cos-squared of the waveplate angle. 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/7912c87c-0fde-4aa7-9e4c-939ec796307c)

and making a histogram (in this section, I am not looking at real data)

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/04351788-5479-484a-b49d-589df08b1465)

or KDE plot

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/5d39b9c7-d799-4ede-8592-30ab3339417d)

We can see that there is a bias in which energies are being explored. Nathaniel told me that the actual angle on the WP and the angle to the laser axis are not necessarily the same. This is why the MP and PP distributions seem to be flipped (maybe there is around a 90 degree offset from the laser axis) and this is something that can be calibrated the day of. My proposed solution is to find out wherever the maximum energy is (for the main pulse) and minimum energy is (for the prepulse) and store those angles as reference angles. At these maximum (or minimum) angles we are oversampling the most and would need to slow down the rate of data collection. I can demonstrate my solution with an example:

Say, we have a total of N = 100 points between a max (or min) angle and the ending angle (which may be something like 15-30 degrees off from the max/min angle) during a time $T$. We'll call this angular range $\Delta \theta$. If we were moving at the normal linear rate, we'd have a step size of $\Delta \theta/N$. Instead, for the first $T/3$ seconds we should move with a step size $5 \Delta \theta/(3N)$, the second $T/3$ seconds we should move with a step size $\Delta \theta/N$, and the last $T/3$ seconds we should move with step size $\Delta \theta/(3N)$. In this way, the larger step size initially means that we are sampling fewer points in that initial region we were oversampling. I can plot a histogram of intensities sampled this way as: 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/30086b48-bcff-4da7-ae34-955639d0d872)

which is much closer to a uniform distribution.

# Fixing Mightex Shot Counts

I came up with a simple fix to revise the shot counts in the following code

```python
i = 1 # index of x
while (i < len(x)):
    if x[i] < x[i-1]:
        x[i:] += 2**16
    i += 1
```
this will just detect any time we go from higher to lower shot count (when we overflow past the $2^{16}$ limit) and add that limit to the rest of the numbers in the sequence. This runs in a fraction of a second with millions of shots.

# Comparing Linear and Gaussian Process Regression on Data

Building on what I did earlier, I plotted the scatter plot of the data with the main pulse and prepulse on the x and y axes colored by the electron number

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/6d4ff7ca-3cb6-4204-9fc1-cdfb22a8857f)

and median pixel number

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/ceef03c5-7374-494b-a31c-65400c3d8499)

Here, I used a 5th degree polynomial and a GPR with $\alpha = 2$ because that is what worked best with `GridSearchCV`. We can see that both models effectively smooth out the data and their testing accuracies are 3.71 and 3.69 for polynomial and gpr respectively (MAPE). However, there seems to be gaps in the parameter space. This is most likely because I removed all entries that had less than 1 million electron counts which resulted in a dataset of size 3773. I can rerun this same analysis with the minimum electron counts at 10000 instead which has dataset size 23,367. We can see the inherent issue in the correlation plot: 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/f88c9ea6-9912-49c3-b5c6-bc432b95084b)

below an electron ccd count of around 100,000, there appears to be no correlation of the electron count with any of the other parameters (see the flat bottom in the bottom row of the plots). I think that without a sufficient number of electron counts, we don't get a clear electron signal peak and our models will be useless in its predictions. The testing error was around 18.67 percent (significantly higher). However, we can see that some of the gaps are filled in

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/3ae35da6-0adb-4dd3-b258-bdf97df18945)

As a final comparison, I will restrict the electron counts to be over 100,000 for completeness. Here, there are 8,032 total points in the data set with testing errors of around 6.46 and 6.42 for the polynomial and gpr respectively. Here are the plots for the electron count and median pixel

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/232e7825-1cf0-4baf-94ed-fa5920973285)

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/3fa15647-7c72-443a-a33c-abc43ac6d1b6)






























