---
layout: post
title: Log-Scaling Correction, Grid Searches
use_math: true
category: journal
---

# Log-Scaling Correction

## Log-Scaling Correction Derivation
As I've mentioned [previously](https://ronak-n-desai.github.io/24spr3/), the log-scaling skews the predictions of a ML model. In fact, it always makes the ML model under-predict the true function. 
The log normal distribution (see [wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution) page) is what I am using to add noise to the output energies. The relevant formula is 

\begin{equation}
  \mu = \log\left(\frac{\mu_X^2}{\sqrt{\mu_X^2 + \sigma_X^2}}\right)
\end{equation}

Here, $\mu_X$ would be the value of the output energy. After log-scaling, the model will ideally learn the output energy to be $\mu$. When we exponentiate the model prediction, we will be left with $\exp(\mu) < \mu_X$. This is the origin of the under-prediction

The PDF of the log-normal distrubution is 

\begin{equation}
  f(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \exp\left(- \frac{(\log(x) - \mu)^2}{2 \sigma^2} \right)
\end{equation}

where $\mu = \log\left(\frac{\mu_X}{\sqrt{1 + \alpha^2}} \right)$ is the mean of the logged distribution, $\sigma^2 = \log(1 + \alpha^2)$ is the variance of the logged distribution, and $\alpha = \sigma_X / \mu_X$ is the noise level of the original (log-normal) distribution. Additionally, $\mu_X$ and $\sigma_X$ are the mean and variance of the log-normal distribution. 

The expected value $E[x] = \mu_X$ can be confirmed by taking $\int_0^\infty x f(x)$ (I just used Mathematica). To get the expected value of the log of this distribution, we can take $\int Log(x) f(x) = \log\left(\frac{\mu_X}{\sqrt{1 + \alpha^2} } \right) = \mu$ which makes sense due to our definition of $\mu$. We can calculate the underprediction bias as a percentage as

\begin{equation}
\text{bias} = \frac{\mu_X - \mu^*}{\mu_X} \times 100 \% = \frac{\sqrt{1 + \alpha^2} - 1}{\sqrt{1 + \alpha^2}} \times 100 \% \label{eq:bias}
\end{equation}

where $\mu^* \equiv e^\mu$

This bias is something that is discussed online. A simple explanation can be given in this [Medium](https://roizner.medium.com/when-logarithmic-scale-in-prediction-models-causes-bias-d80d84e9e3d5) article. An academic article is given by [Miller 1984](https://www.jstor.org/stable/2683247?origin=crossref&seq=2)

## Techniques to Implement Prediction Correction
To fix this underprediction bias, we would want to multiply the predictions by a correction factor equal to $\mu_X / \mu^* = \sqrt{1 + \alpha^2}$. However, this comes with two assumptions. 

1. The noise in our experiment is distributed log-normally
2. We know the constant noise level $\alpha$ exactly.

This multiplicative factor of $\sqrt{1 + \alpha^2}$ should work for our synthetic data, but may be difficult to apply to real experimental data where the amount of noise and noise model may not be known exactly or hard to determine. However, the important thing is that in this process we are developing a correction factor that is equal to the ratio of two quantities: (1) the mean of the un-logged distribution, (2) the exponential of the mean of the logged distribution. We can determine (1) from the experimental dataset and (2) from the training predictions on the experimental dataset. I explored three ways to come up with an effective correction factor $C$. I will refer to the predictions as $y_P$ and the data as $y_D$. 

1. C = mean($y_D$) / mean($y_P$)
2. C = mean($y_D / y_P$)
3. Perform a linear least-squares fit $y_D = C y_P$ (setting the y-intercept to 0) to determine $C$

All three of these methods performed very similarly:

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/ceeef52a-53ff-476e-9d04-205886559d11)

The black dashed line shows the log-scaling bias from eq. \ref{eq:bias}. The gray dashed line shows the MAPE between the polynomial predictions and the exact energy (which is clearly affected by the log-scaling bias). 

In red, green, blue, we have the MAPE between the corrected polynomial predictions and exact energy from methods 1, 2, and 3 respectively. I plotted them with different line widths so that they are all visible together due to overlapping entirely. All of them perform pretty much the same and are able to have much lower MAPEs than the log-scaling bias curve. By the way, I developed the correction factor on the training set only, and used that correction factor to correct the testing set predictions for this plot.

To assess which ones if any are better, I looked at all 420 comparisons with the testing set (7 noise levels from 0 to 30, 3 energy outputs of max total average, 20 different training data points from 1000-20000) and counted which of the three methods had the lowest MAPE. Method 1 won 133 times, Method 2 won 206 times, Method 3 won 81 times. If I convert this to a percentage this is 

1. $31.7 \%$
2. $49.0 \%$
3. $19.3 \%$

So, it appears from this analysis that method 2 is a sensible choice to stick with. And, to me, it makes the most sense because it is averaging data point by data point this bias ratio. 

# Looking through data at WPAFB from 02/26/24

## Diode Data (Main Pulse and Prepulse)
The diode data will be stored in files that look like the following: 

`DIODE-H7-00-2024-02-26_11h30m15s111ms.h5`

where `00` would correspond to the main pulse stored in `DAQ1` and `01` would correspond to the prepulse stored in `DAQ2`. In each file, there are two categories: `Metrics` and `Trace`. 

The `Trace` values include the following (in parenthesis are the dimensions of the data): 
- shot_num(x): self-explanatory
- shot_time_seconds(x): only rounds to nearest second
- shot_time_nanos(x): shows the fractional part of the shot time (in nanoseconds). i.e. shot_time_seconds + shot_time_nanos/1e9 is the actual time.
- yvals(x, 1000): plots trace of a shot over 1ms with spacing 1$\mu$s (hence the 1000 points). 

The traces are like an oscilloscope trace, recording the intensity over a certain time interval. Scott said that we should be seeing (for yvals) a peak followed by a sharp exponential decay for the signals at each trace data point. For example, here's the Main Pulse trace at the 25000th data point (`yvals[25000, :])`):

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/a1844ec1-dc43-4967-a1e3-1be0b8a9d004)

The diodes are recording this data for each shot, but it is not necessary for our purposes to have the full trace data. Really, we just need to extract one singular value from this trace which will be proportional to the intensity of the laser. This is what the `Metrics` category is useful for. As a result, not every trace is saved which can be seen by the `shot_num` increasing in increments of 250. Additionally, since there aren't a fixed number of data points per file, I left the first dimension as $x$.

The `Metrics` values include the following:
- bg_mean (29,997): Presumably the mean of the background
- reduced_mean (29,997): Presumably the mean of the intensity value that we want extracted from the traces
- shot_num (29,997)
- shot_time_alt_nanos (29,997): not sure what exactly this is
- shot_time_nanos (29,997)
- shot_time_seconds (29,997)

The `Metrics` data are outputted every 30s which approximately gives us 1000 shots per second of data.

## Mightex Data (Pspec and Espec)
The Mightex data will be stored in files that look like the following:

`MIGHTEX1-2024-02-26_08h42m14s900ms.h5`

where 1 indicates the proton spectrometer (Pspec) and 2 indicates the electron spectrometer (Espec). Both of these are stored in the `XL-TO-GO` network drive. There are only two relevant pieces of data that we will need to examine here: `TProcessedData` and `yvals`. 

The `TProcessedData` itself has shape (x, 8) which are 8 separate quantities of interest (index 0-7)
0. CameraID
1. ExposureTime
2. TimeStamp: Between 0-65535 in untis of 100 $\mu$s. Therefore, it resets every 6.55 seconds.
3. TriggerOccured: 
4. TriggerEventCount: Shows trigger event counts. Should be increasing in time (I apparently didn't enable trigger counts this data collection run)
5. OverSaturated
6. LightShieldPixelAverage
7. GlobalGain
8. FrameTime

I believe the FrameTime is the one that is missing from our spectrometer data (but I believe it was in Nathaniel's Data because he had 9 values in TProcessedData for his 2022 data).

The `yvals` data has shape (x, 3648) because there are 3648 pixels on the line CCD camera for both the Pspec and Espec. Particles with different speeds will arrive at different pixels according to its deflection from a magnetic field. Chris emailed me some matlab code which details this mapping and Nathaniel has a simplified approach that involves just classically considering the deflection of a charged particle in a constant magnetic field with no electric field. The proton spectrometer doesn't appear to be getting any counts so that is something that might need to be fixed. But here is some preliminary analysis on the electron spectrometer data. 

First, I plotted an average of the `yvals` counts over each pixel to understand which points (of the 199118 total data points) actually had appreciable electron counts. 

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/115aba3e-5bff-4004-811e-b29518ec509e)

One such area is around data point 75,000. I plotted the mean pixel count between 70,000 and 80,000 data points below to look at the electron spectrum

![image](https://github.com/ronak-n-desai/ronak-n-desai.github.io/assets/98538788/9c918959-d7d2-42c2-a472-b82d56f1099d)


## Target Values
The targets also get stored in h5 files that look like

`TARGET-2024-02-26_08h42m14s900ms.h5`

and 7 different values are being tracked which aren't multi-dimensional and it appears they get logged every 5 or so seconds when we put the setting on StarDestroyer as 20 frames/save. 

1. IR - Y:
2. IR - Z:
3. Nozzle Focus
4. Nozzle Horizontal
5. Nozzle Rotation
6. Nozzle Vertical
7. e-Spec Pitch

However, none of the target values changed during our collection time. Ideally, we would only need to change one value (maybe Nozzle Vertical) to change the thickness of the target. One issue with this file is that there is no timestamp or shot_num associated with it. I can extract the time that the file was last modified which might be okay for now, but may not be exact. In that case, lowering to 1 frame/save would work, but also generate a ton of files.

## Pre-Pulse Delay
The pre-pulse delay time is simply outputted as a text file that contains two things: time and the delay (in picoseconds). Each line looks something like this:

`1:50:55 PM    -2000.0000`






